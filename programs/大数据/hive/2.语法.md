[[TOC]]

# 数据类型
基本数据类型：int、float、double、string、boolean、bigint等
复杂类型：array、map、struct

# hive分区
hive将海量数据按某几个字段进行分区，查询时不必加载全部数据
分区对应到HDFS就是HDFS的目录
分区分为静态分区和动态分区两种（不懂）

# 语法
use database_name
create database if not exists db_name
desc database db_name

指定分隔符

create table table_name(...) row format delimited fields terminated by "\t" store as textfile
指定分隔符是\t
指定存储格式是textfile

select * from table_name

alter table table_name rename to new_table_name


# hive命令
hive -e "show databases"
hive -f test.sql


create table test(
    user_id string,
    user_name string,
    hobby array<string>,
    scores map(string,integer)
)
row format delimited
fields terminated by '|'
collection items terminated by ','
map keys terminated by ':'
lines terminated by '\n';

load data local inpath '/home/hadoop/test.csv' overwrite into table test;

beeline -u jdbc:hive2://localhsot:10000 -u hadoop -e "select...."

# hive 基本使用
hive目录
# hive数据类型
内部表：和传统数据库的Table概念类似，对应HDFS上存储目录，删除表时，删除元数据和表数据
外部表：指向已经存在的HDFS数据，删除时只删除元数据信息

创建内部表
```
create table test(
    id string,
    value string
)
row format delimited
fields terminated by '|'
map keys terminated by ':'
lines terminated by '\n'

load data inpath '/test/test.csv' overwrite into table test;

hdfs dfs -ls /test
```

创建外部表
```
create external table external_table(
    id string,
    value string
)
row format delimited
fields terminated by '|'
collection items terminated by ','
map keys terminated by ':'
lines terminated by '\n'

load data inpath '/test/test.csv' overwrite into table test;

hdfs dfs -ls /hive/warehouse/hive_test.db/external_table
```

删除内部表：hdfs内容会被删除
删除外部表：数据在，表不在


分区表：Partition对应普通数据库对Partition列的密集索引，将数据按照Partition列存储到不同目录，便于并行分析，减少数据量

分桶表：对数据进行hash，放到不同文件存储，方便抽样和join查询

分区：
```
create table partition_table(
    id string,
    value string
)
partitioned by (time string)
row format delimited
fields terminated by '|'
collection items terminated by ','
map keys terminated by ':'
lines terminated by '\n';

load data local inpath '/test.csv' overwrite into table partition_table partition (time='201911')
load data local inpath '/test.csv' overwrite into table partition_table partition (time='201912')

hdfs dfs -ls /hive/warehouse/hive_test.db/partition_table
time=201911
time=201912
```

分桶：
```
create table bucket_table(
    id string,
    value string
)
clustered by (value) sorted by (value) into 2 buckets
row format delimited
fields terminated by '|'
collection items terminated by ','
map keys terminated by ':'
lines terminated by '\n';

insert into bucket_table select * from test;

select * from bucket_table tablesample(bucket 1 out of 2 on value)
```