[TOC]

# hive存储格式
|              |         优点         |       缺点       |
| ------------ | -------------------- | ---------------- |
| TextFile     | 简单、方便查看        | 不支持分片        |
| SequenceFile | 可压缩、可分割        | 需要合并、不易查看 |
| OrcFile      | 分片、按列查询、速度快 | 不易查看          |


# 行存储，列存储



|     |                       行式存储                        |                                  列式存储                                  |
| --- | ---------------------------------------------------- | ------------------------------------------------------------------------- |
| 优点 | 数据被保存在一起</br>insert/update容易                 | 查询时只有涉及到的列会被读取</br>投影（projection）很高效</br>任何列都能作为索引 |
| 缺点 | 选择（selection）时即使只涉及某几列，所有数据也都会被读取 | 选择完成时，被选择的列要重新组装</br>insert/update比较麻烦                     |

传统行式数据库
+ 数据市按行存储的
+ 没有索引的查询使用大量I/O
+ 建立索引和物化视图需要花费大量时间和资源
+ 面对查询的需求，数据库必须被大量膨胀才能满足性能要求

列式数据库
+ 数据按列存储，每一列单独存放
+ 数据即是索引
+ 只访问查询涉及的列：大量降低系统IO
+ 每一列由一个线索来处理：查询的并发处理
+ 数据类型一致，数据特征相似：高效压缩

列式存储优点
+ 查询时只需要读取查询所涉及的列，降低IO消耗，同时保存每一列统计信息，实现部分谓词下推
+ 每列数据类型一致，可针对不同的数据类型采用其高效的压缩算法
+ 列式存储格式假设数据不会发生变化，支持分片、流式读取，更好的适应分布式文件存储的特性

# Orc vs Parquet
+ OrcFile和Parquet都是Apache的顶级项目
+ Parquet不支持ACID、不支持更新，Orc支持有限的ACID和更新
+ Parquet的压缩能力较高，Orc的查询效率较高


# OrcFile

和Parquet类似，ORC文件也是以二进制方式存储的，所以是不可以直接读取，ORC文件也是自解析的，它包含许多的元数据，这些元数据都是同构ProtoBuffer进行序列化的。ORC的文件结构如下图，其中涉及到如下的概念：
+ ORC文件：保存在文件系统上的普通二进制文件，一个ORC文件中可以包含多个stripe，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。
+ 文件级元数据：包括文件的描述信息PostScript、文件meta信息（包括整个文件的统计信息）、所有stripe的信息和文件schema信息。
+ stripe：一组行形成一个stripe，每次读取文件是以行组为单位的，一般为HDFS的块大小，保存了每一列的索引和数据。
+ stripe元数据：保存stripe的位置、每一个列的在该stripe的统计信息以及所有的stream类型和位置。
+ row group：索引的最小单位，一个stripe中包含多个row group，默认为10000个值组成。
+ stream：一个stream表示文件中一段有效的数据，包括索引和数据两类。索引stream保存每一个row group的位置和统计信息，数据stream包括多种类型的数据，具体需要哪几种是由该列类型和编码方式决定。


![](https://gitee.com/caijingquan/imagebed/raw/master/20210419002926.jpg)



Hive知识梳理总结
大家好，通过前面几节Hive相关的小节，我们已经部署了Hive环境，并且成功的进行了实战。对Hive的基础语法、存储模型和自定义UDF进行了实战。通过我们的实战操作，相信大家对于Hive已经有了一定的了解了。
本文我们就一起来归纳总结一下Hive相关的一些知识点，为前面的实战章节查漏补缺，在对Hive熟练操作的情况下对理论知识也有所了解。

一句话描述Hive
Hive是一个构建在Hadoop之上的数据仓库软件,它可以使已经存储的数据结构化，它提供类似sql的查询语句HiveQL对数据进行分析处理。

Hive将HQL转换为MapReduce的流程
了解了MapReduce实现SQL基本操作之后，我们来看看Hive是如何将SQL转化为MapReduce任务的，整个编译过程分为六个阶段：

Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree
遍历AST Tree，抽象出查询的基本组成单元QueryBlock
遍历QueryBlock，翻译为执行操作树OperatorTree
逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量
遍历OperatorTree，翻译为MapReduce任务
物理层优化器进行MapReduce任务的变换，生成最终的执行计划
Hive与关系数据库Mysql的区别
产品定位
Hive是数据仓库，是为海量数据的离线分析设计的，不支持OLTP(联机事务处理)所需的关键功能ACID，而更接近于OLAP(联机分析技术)，适合离线处理大数据集；
而MySQL是关系型数据库，是为实时业务设计的。
可扩展性
Hive中的数据存储在HDFS(Hadoop的分布式文件系统)，metastore元数据一般存储在独立的关系型数据库中，而MySQL则是服务器本地的文件系统；因此Hive具有良好的可扩展性，数据库由于ACID语义的严格限制，扩展性十分有限。
读写模式
Hive为读时模式，数据的验证则是在查询时进行的，这有利于大数据集的导入，读时模式使数据的加载非常迅速，数据的加载仅是文件复制或移动。MySQL为写时模式，数据在写入数据库时对照模式检查。写时模式有利于提升查询性能，因为数据库可以对列进行索引。
数据更新
Hive是针对数据仓库应用设计的，而数仓的内容是读多写少的，Hive中不支持对数据进行改写，所有数据都是在加载的时候确定好的；而数据库中的数据通常是需要经常进行修改的。
索引
Hive支持索引，但是Hive的索引与关系型数据库中的索引并不相同，比如，Hive不支持主键或者外键。Hive提供了有限的索引功能，可以为一些字段建立索引，一张表的索引数据存储在另外一张表中。由于数据的访问延迟较高，Hive不适合在线数据查询；数据库在少量的特定条件的数据访问中，索引可以提供较低的延迟。
计算模型
Hive使用的模型是MapReduce(也可以 on spark)，而MySQL使用的是自己设计的Executor计算模
Hive的数据存储格式
Hive中的数据存储格式分为TextFile、SequenceFile和RCFile三种，其中TextFile是默认的存储格式，通过简单的分隔符可以对csv等类型的文件进行解析。而ORCFile是我们常用的一种存储格式，因为ORCFile是列式存储格式，更加适合大数据查询的场景。

Hive表类型
Hive几种基本表类型：内部表、外部表、分区表、分桶表。

内部表：内部表的数据，会存放在 HDFS 中的特定的位置中，我们在安装Hive的配置中是在/hive/warehouse。；当删除表时，数据文件也会一并删除；适用于临时创建的中间表
外部表：适用于想要在 Hive 之外使用表的数据的情况．当你删除 External Table 时，只是删除了表的元数据，它的数据并没有被删除。适用于数据多部门共享。建表时使用create external table。 指定external关键字即可。
分区表：分区表创建表的时候需要指定分区字段，分区字段与普通字段的区别：分区字段会在HDFS表目录下生成一个分区字段名称的目录，而普通字段则不会，查询的时候可以当成普通字段来使用，一般不直接和业务直接相关。
分桶表：将内部表，外部表和分区表进一步组织成桶表，可以将表的列通过Hash算法进一步分解成不同的文件存储。

对于内部表和外部表的概念和使用我们很容易理解，我们需要重点关注一下分区表和分桶表。我们为什么要建立分区表和分桶表呢？HQL通过where字句来限制条件提取数据，那么遍历一张大表，不如将这张大表拆分成多个小表，并通过合适的索引来扫描表中的一小部分，分区和分桶都是采用了这种理念。

分区会创建物理目录，并且可以具有子目录(通常会按照时间、地区分区)，目录名以=创建，分区名会作为表中的伪列，这样通过where字句中加入分区的限制可以在仅扫描对应子目录下的数据。通过partitioned by(field1 type,…)
分桶可以继续在分区的基础上再划分小表，分桶根据哈希值来确定数据的分布(即MapReducer中的分区！)，比如分区下的一部分数据可以根据分桶再分为多个桶，这样在查询时先计算对应列的哈希值并计算桶号，只需要扫描对应桶中的数据即可。通过 clustered by( field ) into n buckets

Hive自定义函数
当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数，Hive中包含三类自定义函数：
UDF：普通的用户自定义函数。接受单行输入，并产生单行输出。如转换字符串大小写，获取字符串长度等
UDAF：用户定义聚集函数（User-defined aggregate function）。接受多行输入，并产生单行输出。比如MAX，COUNT函数。
UDTF：用户定义表生成函数（User-defined table-generating function）。接受单行输入，并产生多行输出（即一个表），不是特别常用

关于UDF的开发我们在视频中已经进行了演示，大家还是要自己多多实践一下。


3-12