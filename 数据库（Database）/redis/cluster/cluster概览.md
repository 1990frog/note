[TOC]

# 为什么要集群
+ 并发量
+ 数据量
+ 网络

打个比方：如果一辆马车太重了，一匹马拉不动，那换一只大象来拉，那如果大象也拉不动呢
团队的力量

# 顺序分区和哈希分区
| 分布方式 |                           特点                            |              典型产品               |
| ------ | -------------------------------------------------------- | --------------------------------- |
| 哈希分布 | 时间分散度高</br>键值分布业务无关</br>无法顺序访问</br>支持批量操作 | 一致性哈希Memcache</br>Redis Cluster |
| 顺序分布 | 数据分散度易倾斜</br>键值业务相关</br>可顺序访问</br>支持批量操作   | BigTable</br>HBase                 |

# 哈希分布三种实现方式
+ 节点取余分区
+ 一致性哈希分区
+ 虚拟槽分区

# 节点取余分区
hash(key)%nodes
如果增加了一个节点，会造成数据漂移

+ 客户端分片：哈希+取余
+ 节点伸缩：数据节点关系变化，导致数据迁移
+ 迁移数量和添加数量有关：建议翻倍扩展

# 一致性哈希
一致性 Hash 算法也是使用取模的思想，只是，刚才描述的取模法是对节点数量进行取模，而一致性Hash算法是对 2^32 取模，什么意思呢？简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希环如下，从 0 ~ 2^32-1 代表的分别是一个个的节点,这个环也叫哈希环
![9463862-802d18815e5cb1ae](https://raw.githubusercontent.com/1990frog/imagebed/default/1602320380_20200321152505266_78462303.png)

然后我们将我们的节点进行一次哈希，按照一定的规则，比如按照 ip 地址的哈希值，让节点落在哈希环上。比如此时我们可能得到了如下图的环:

![9463862-e786b08fc6b0d7fd](https://raw.githubusercontent.com/1990frog/imagebed/default/1602320380_20200321152543456_1271541566.png)

然后就是需要通过数据 key 找到对应的服务器然后存储了，我们约定,通过数据 key 的哈希值落在哈希环上的节点，如果命中了机器节点就落在这个机器上，否则落在顺时针直到碰到第一个机器。如下图所示 : A 的哈希值落在了 D2 节点的前面，往下找落在了 D2 机器上，D的哈希值 在 D1 节点的前面，往下找到了 D1 机器，B的哈希值刚好落在了D1 节点上，依次~~~

![9463862-210f9b16052620ec](https://raw.githubusercontent.com/1990frog/imagebed/default/1602320381_20200321152625376_480410324.png)

## 一致性哈希的分析
一致性哈希主要就是解决当机器减少或增加的时候，大面积的数据重新哈希的问题，主要从下面 2 个方向去考虑的，当节点宕机时，数据记录会被定位到下一个节点上，当新增节点的时候 ，相关区间内的数据记录就需要重新哈希。

某节点宕机
我们假设上图中的 节点 D2 因为一些原因宕机了,可以看到，只有数据 A 的记录需要重新重新定位存储到节点 D1 上，因为 D1 是 D2 的下一个节点，其它的数据都没有被影响到，此时被影响的仅仅是 图中的 D0-D2 这段区间的记录，也就是之前落在 D2 上的数据现在都要落到 D1 上面了。如下图

![9463862-ee9c9c42f0bf764f](https://raw.githubusercontent.com/1990frog/imagebed/default/1602320382_20200321152806508_164953606.png)




客户端分片：哈希+顺时针（优化取余）
节点伸缩：只影响临近的节点，但是还是有数据迁移
翻倍伸缩：保证最小迁移数据和负债均衡

# 虚拟槽分区（Redis）
+ 预设虚拟槽：每个槽映射一个数据子集，一般比节点数大
+ 良好的哈希函数：例如CRC16
+ 服务端管理节点、槽、数据：Redis Cluster

Redis 集群包含了`16384`个哈希槽，每个Key经过计算后会落在一个具体的槽位上，而槽位具体在哪个机器上是用户自己根据自己机器的情况配置的，机器硬盘小的可以分配少一点槽位，硬盘大的可以分配多一点。如果节点硬盘都差不多则可以平均分配。

所以哈希槽这种概念很好地解决了一致性哈希的弊端。

另外在容错性和扩展性上与一致性哈希一样，都是对受影响的数据进行转移而不影响其它的数据。

而哈希槽本质上是对槽位的转移，把故障节点负责的槽位转移到其他正常的节点上。扩展节点也是一样，把其他节点上的槽位转移到新的节点上。

需要注意的是，对于槽位的转移和分派，Redis集群是不会自动进行的，而是需要人工配置的。所以Redis集群的高可用是依赖于节点的主从复制与主从间的自动故障转移。


