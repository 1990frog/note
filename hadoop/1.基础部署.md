

# hadoop 基础环境
[下载](https://archive.apache.org/dist/hadoop/common/)

版本选择：3.1.4

## 设置环境变量
```sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk
export JRE_HOME=${JAVA_HOME}/jre
export PATH=${JAVA_HOME}/bin:$PATH
export HADOOP_HOME=/usr/local/hadoop
export PATH=${HADOOP_HOME}/bin:/sbin:$PATH
```

配置文件位置 `/etc/hadoop/`

## 编辑 hadoop-env.sh 文件
```sh
# 添加JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk
```

## namenode
 Hadoop Namenode 是 Hadoop 分布式文件系统（HDFS）的核心组成部分，它负责管理文件系统的命名空间和数据的复制。

## 编辑 core-site.xml 集群全局参数
```xml
<configuration>
    <!-- 指定namenode地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://0.0.0.0:9000</value>
    </property>
    <!-- 用来指定使用hadoop时产生文件的存放目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/home/cai/App/hadoop/tmp/name</value>
    </property>
    <!-- 配置该superUser允许通过代理访问的主机节点 -->
    <property>
        <name>hadoop.proxyuser.cai.hosts</name>
        <value>*</value>
    </property>
    <!-- 配置该superUser允许代理的用户所属组 -->
    <property>
        <name>hadoop.proxyuser.cai.groups</name>
        <value>*</value>
    </property>
    <!-- 配置该superUser允许代理的用户 -->
    <property>
        <name>hadoop.proxyuser.cai.users</name>
        <value>用户1,用户2</value>
    </property>
</configuration>
```

## 编辑hdfs-site.xml
hdfs
```xml
<configuration>
    <!--指定hdfs保存数据的副本数量-->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <!--指定hdfs中namenode的存储位置-->
    <property>
        <name>dfs.namenode.name.dir</name> 
        <value>file:/home/cai/App/hadoop/tmp/name</value>
    </property>
    <!--指定hdfs中datanode的存储位置-->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/cai/App/hadoop/tmp/data</value>
    </property>
    <property>  
        <name>dfs.webhdfs.enabled</name> 
        <value>true</value>
    </property> 
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>20</value>
    </property>
</configuration>
```

## mapred-site.xml
mapreduce
```xml
<configuration>
    <!--告诉hadoop以后MR(Map/Reduce)运行在YARN上-->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

## yarn-site.xml
yarn
```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

# HDFS 格式化
```sh
# 格式化
> hdfs namenode -format
# 设置权限
> sudo chmod -R 777 /home/cai/App/hadoop/tmp
```


<!-- # bug

## localhost: ssh: connect to host localhost port 22: Connection refused
未安装openssh,或未启动

## localhost: cai@localhost: Permission denied (publickey,password).
ssh-keygen -t rsa -P ''
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

## /usr/local/hadoop/sbin/start-dfs.sh: 行 129: hostname: 未找到命令
sudo pacman -S net-tools -->

# hive

## 设置环境变量 hive-env.sh
```sh
export HADOOP_HOME=/usr/local/hadoop
export HIVE_CONF_DIR=/usr/local/hive/conf
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk
```

## 编辑 hive-site.xml
```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration> 
	<!--mysql地址-->
	<property> 
		<name>javax.jdo.option.ConnectionURL</name>  
		<value>jdbc:mysql://localhost:3306/metastore?createDatabaseIfNotExist=true</value>  
		<description>the URL of the MySQL database</description> 
	</property>
	<!--JDBC驱动-->
	<property> 
		<name>javax.jdo.option.ConnectionDriverName</name>  
		<value>com.mysql.cj.jdbc.Driver</value>  
		<description>Driver class name for a JDBC metastore</description> 
	</property>
	<!--用户名-->
	<property> 
		<name>javax.jdo.option.ConnectionUserName</name>  
		<value>root</value> 
	</property>
	<!--密码-->
	<property> 
		<name>javax.jdo.option.ConnectionPassword</name>  
		<value>root</value> 
	</property>
	<!---->
	<property> 
		<name>hive.metastore.warehouse.dir</name>  
		<value>/home/cai/App/hive/warehouse</value> 
	</property>
	<!---->
	<property> 
		<name>hive.exec.scratchdir</name>  
		<value>/home/cai/App/hive/tmp</value> 
	</property>
	<!---->
	<property> 
		<name>hive.querylog.location</name>  
		<value>/home/cai/App/hive/log</value> 
	</property>
	<!---->
	<property> 
		<name>hive.metastore.schema.verification</name>  
		<value>false</value> 
	</property>
	<property> 
		<name>datanucleus.autoCreateSchema</name> 
		<value>true</value> 
	</property> 
	<!---->
	<property> 
		<name>datanucleus.schema.autoCreateAll</name> 
		<value>true</value> 
	</property>
	<property>
		<name>hive.server2.thrift.host</name>
		<value>172.17.0.1</value>
	</property>
	<property>
		<name>hive.server2.thrift.port</name>
		<value>10000</value>
	</property>
	<property>
		<name>hive.server2.enable.doAs</name>
		<value>faslse</value>
	</property>
</configuration>

```