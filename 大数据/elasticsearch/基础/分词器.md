[TOC]

# 分词器组成
分词器是es中专门处理分词的组件，英文是Analyzer，它的组成如下：
+ Character Filters：针对原始文本进行处理，比如去除html特殊标记符
+ Tokenizer：将原始文本按照一定规则切分为单词
+ Token Filter：针对tokenizer处理的单词进行再加工，比如转小写、删除或新增等处理

# _analyze API
es提供了一个测试分词的api接口，方便验证分词效果，endpoint是_analyze
+ 可以直接指定analyzer进行测试
+ 可以直接指定索引中的字段进行测试
+ 可以自定义分词器进行测试

直接测试
```json
POST _analyze
{
  "analyzer":"standard",# 分词器
  "text":"hello world!"# 测试文本
}

{
  "tokens" : [
    {
      "token" : "hello",# 分词结果
      "start_offset" : 0,# 起始偏移
      "end_offset" : 5,# 结束偏移
      "type" : "<ALPHANUM>",
      "position" : 0#分词位置
    },
    {
      "token" : "world",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```

根据指定索引测试
```json
POST index/_analyze
{
  "field":"standard",
  "text":"hello world!"
}
```

自定义分词器进行测试，自定义分词器三个组件
```json
POST _analyze
{
  "tokenizer":"standard",
  "filter":["lowercase"],
  "text":"hello world!"
}
```

# es预定义分词器
+ Standard：默认分词器，按词切分，小写处理
+ Simple：按照非字母切分（符号被过滤），小写处理
+ Whitespace：按照空格切分，不转小写
+ Stop：小写处理，停用词过滤（the，a，is）
+ Keyword：不分词，直接将输入当做输出
+ Pattern：正则表达式，默认\w+（非字符分割）
+ Language：提供了30多种常见语言的分词器
+ Customer：自定义分词器

# 中文分词
难点
中文分词指的是讲一个汉字序列分成一个一个单独的词。在英文中，单词之间是以空格作为自然分解符，汉语中词没有一个形式上的分界符。

上下文不同，分词结果迥异，比如交叉歧义问题，比如下面两种分词都合理
乒乓球拍/卖/完了
乒乓球/拍卖/完了

常用中文分词系统
+ IK [github](https://github.com/medcl/elasticsearch-analysis-ik)
+ jieba
+ Hanlp
+ THULAC

# Tokenizer
将原始文本按照一定规则切分为单词（term or token）

自带的如下：
+ standard按照单词进行分割
+ letter按照非字符类进行分割
+ whitespace按照空格进行分割
+ UAX URL Email按照standard分割，但不会分割邮箱和url
+ NGram和Edge NGram连词分割
+ Path Hierarchy按照文件路径进行切割

```json
POST _analyze
{
  "tokenizer":"letter",
  "text":"aaaaaaaa"
}
```

# Token Filters
对于tokenizer输出的单词（term）进行增加、删除、修改等操作

自带的如下：
+ lowercase将所有term转换为小写
+ stop删除stop words
+ NGram和Edge NGram连词分割
+ Synonym添加近义词的term

# 自定义分词
自定义分词需要在索引的配置中设定，如下：
```json
PUT index
{
  "settings":{
    "analysis":{
      "char_filter":{},
      "tokenizer":{},
      "filter":{},
      "analyzer":{}
    }
  }
}
```

# 分词使用的两个时机
+ 创建或更新文档时
+ 查询时

查询时分词的指定方式：
+ 查询时通过analyzer指定分词器
+ 通过index mapping设置search_analyzer实现

# 分词的使用建议
+ 明确字段是否需要分词，不需要分词的字段就将type设置为keyword，可以节省空间和提高写性能
+ 善用_analyze API，查看文档的具体分词结果


# 测试各种分词器效果
```json
GET _analyze
{
    "analyzer":"keyword",
    "text":"sdkfjslkdfjaskfdklajldfklsajldfaljk"
}
```